{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting data pipeline...\n",
      "INFO:__main__:Extracted 411547 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction step complete\n",
      "Initiating data cleaning\n",
      "Data shape: (411547, 4)\n",
      "\n",
      "Sample data:                                                title  \\\n",
      "0  AppRiver welcomes Kevin Hatch as Chief Financi...   \n",
      "1  Italy president names ex-IMF official as inter...   \n",
      "2  Nobel Biocare CEO aims for growth, not M&A: paper   \n",
      "3  Atria Wealth Solutions Acquires Cadaret, Grant...   \n",
      "4  HSBC third-quarter profit rises to $5 billion,...   \n",
      "\n",
      "                           Date  \\\n",
      "0                           NaN   \n",
      "1                           NaN   \n",
      "2  Sun Aug 26, 2012 10:26am EDT   \n",
      "3                           NaN   \n",
      "4    Mon Nov 4, 2013 9:20am EST   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://www.cnbc.com/2018/01/24/globe-newswire-...   \n",
      "1  https://www.reuters.com/article/us-italy-polit...   \n",
      "2  http://www.reuters.com/article/2012/08/26/us-n...   \n",
      "3  http://www.cnbc.com/2018/04/19/business-wire-a...   \n",
      "4  http://www.reuters.com/article/2013/11/04/us-h...   \n",
      "\n",
      "                                                text  \n",
      "0  Gulf Breeze, Jan. 24, 2018 (GLOBE NEWSWIRE) --...  \n",
      "1  MILAN (Reuters) - Italyâ€™s president appointed ...  \n",
      "2   The economic downturn has caused patients to ...  \n",
      "3  NEW YORK & SYRACUSE, N.Y.--(BUSINESS WIRE)-- A...  \n",
      "4   Europe's largest bank said underlying pretax ...  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristianzuniga/Library/Python/3.10/lib/python/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/cristianzuniga/Library/Python/3.10/lib/python/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates extracted\n",
      "Transform step complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pipeline completed. Results: {'DatabaseSink': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load step complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from dateutil import parser\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from sqlalchemy import create_engine\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data pipeline\"\"\"\n",
    "    hf_token: str\n",
    "    db_connection_string: str\n",
    "    s3_bucket: str\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 4\n",
    "\n",
    "\n",
    "class DataProcessor(ABC):\n",
    "    \"\"\"Abstract base class for data processors\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "class FinancialNewsProcessor(DataProcessor):\n",
    "    \"\"\"Processes financial news data\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and standardize financial news data\"\"\"\n",
    "        \n",
    "        print('Initiating data cleaning')\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print()\n",
    "        print(f\"Sample data: {data.head()}\")\n",
    "        print()\n",
    "        \n",
    "        # Text length calculation\n",
    "        data['text_length'] = data['text'].apply(len)\n",
    "        \n",
    "        # Extract news medium\n",
    "        data['news_medium'] = (\n",
    "            data[\"url\"]\n",
    "            .str.extract(r\"https?://([^/]+)\")[0]\n",
    "            .str.replace(\"www.\", \"\", regex=False)\n",
    "            .str[:-4]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Standardize dates\n",
    "        data['Date'] = data['Date'].apply(\n",
    "            lambda date: parser.parse(date) if pd.notna(date) else np.NaN\n",
    "        )\n",
    "        \n",
    "        # Extract dates from URLs\n",
    "        url_dates = (data['url']\n",
    "                    .str.extract(r'/(\\d{4})[/\\-](\\d{2})[/\\-](\\d{2})/')\n",
    "                    .dropna()\n",
    "                    .T.apply('-'.join))\n",
    "        \n",
    "        print('Dates extracted')\n",
    "        \n",
    "        data['Date'] = data['Date'].fillna(url_dates)\n",
    "        data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "        data['Date'] = data['Date'].dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Extract news types\n",
    "        data['news_type'] = data['url'].str.extract(r'/([a-zA-Z]+)/')[0]\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "class DataSink(ABC):\n",
    "    \"\"\"Abstract base class for data sinks\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def write(self, data: pd.DataFrame) -> bool:\n",
    "        pass\n",
    "\n",
    "\n",
    "class DatabaseSink(DataSink):\n",
    "    \"\"\"Writes data to analytics database\"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str):\n",
    "        self.engine = create_engine(connection_string)\n",
    "    \n",
    "    async def write(self, data: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            data.to_sql('financial_news', self.engine, if_exists='append', index=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Database write failed: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"Main data pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataConfig):\n",
    "        self.config = config\n",
    "        self.processor = FinancialNewsProcessor()\n",
    "        self.sinks = [\n",
    "            DatabaseSink(config.db_connection_string)\n",
    "        ]\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from multiple sources\"\"\"\n",
    "        \n",
    "        # Load datasets\n",
    "        reuters_df = load_dataset(\"danidanou/Reuters_Financial_News\")['train'].to_pandas()\n",
    "        finnews_df = load_dataset(\"ashraq/financial-news-articles\")['train'].to_pandas()\n",
    "        \n",
    "        # Standardize reuters columns\n",
    "        reuters_df = reuters_df.rename(columns={\n",
    "            'Article': 'text',\n",
    "            'Link': 'url',\n",
    "            'Headline': 'title'\n",
    "        })\n",
    "        \n",
    "        reuters_df = reuters_df.drop(columns=['Journalists', '__index_level_0__', 'Summary'])\n",
    "        \n",
    "        # Merge datasets\n",
    "        merged_df = pd.concat([reuters_df, finnews_df])\n",
    "        \n",
    "        # Shuffle and deduplicate\n",
    "        merged_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
    "        merged_df = merged_df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        self.logger.info(f\"Extracted {len(merged_df)} records\")\n",
    "        return merged_df\n",
    "    \n",
    "    def transform_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform data using processor\"\"\"\n",
    "        return self.processor.process(data)\n",
    "    \n",
    "    async def load_data(self, data: pd.DataFrame) -> Dict[str, bool]:\n",
    "        \"\"\"Load data to multiple sinks concurrently\"\"\"\n",
    "        \n",
    "        # Process in batches\n",
    "        results = {}\n",
    "        \n",
    "        for i in range(0, len(data), self.config.batch_size):\n",
    "            batch = data.iloc[i:i + self.config.batch_size]\n",
    "            \n",
    "            # Write to all sinks concurrently\n",
    "            tasks = []\n",
    "            for sink in self.sinks:\n",
    "                tasks.append(sink.write(batch))\n",
    "            \n",
    "            batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            # Log results\n",
    "            for j, result in enumerate(batch_results):\n",
    "                \n",
    "                sink_name = type(self.sinks[j]).__name__\n",
    "                if sink_name not in results:\n",
    "                    results[sink_name] = []\n",
    "                results[sink_name].append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Starting data pipeline...\")\n",
    "        \n",
    "        # Extract\n",
    "        raw_data = self.extract_data()\n",
    "        print('Extraction step complete')\n",
    "        # Transform\n",
    "        processed_data = self.transform_data(raw_data)\n",
    "        print('Transform step complete')\n",
    "        \n",
    "        # Load\n",
    "        results = await self.load_data(processed_data)\n",
    "        print('Load step complete')\n",
    "        \n",
    "        self.logger.info(f\"Pipeline completed. Results: {results}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# Usage example\n",
    "async def main():\n",
    "    config = DataConfig(\n",
    "        hf_token=os.getenv(\"MACBOOK_HF_KEY\"),\n",
    "        db_connection_string=os.getenv(\"DB_URL\"),\n",
    "        s3_bucket=os.getenv(\"DB_TOKEN\"),\n",
    "        batch_size=1000\n",
    "    )\n",
    "    pipeline = DataPipeline(config)\n",
    "    await pipeline.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e82c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
