{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e45ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import asyncio\n",
    "from scrapy.crawler import Crawler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, proxy, urls) -> None:\n",
    "        self.proxy: str = proxy,\n",
    "        self.urls: list = urls\n",
    "        pass\n",
    "    \n",
    "       \n",
    "    def create_client(self) -> requests.Session:\n",
    "        if self.proxy is None:\n",
    "            raise Exception(\"no proxy available\")\n",
    "        proxies = {\"http\": self.proxy}\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:137.0) Gecko/20100101 Firefox/137.0\"\n",
    "        }\n",
    "        client = requests.Session()\n",
    "        client.proxies.update(proxies)\n",
    "        return client\n",
    "    \n",
    "    \n",
    "    def check_status(self) -> None:\n",
    "        client = create_client()\n",
    "        for url in self.urls:\n",
    "            response = client.get(url)\n",
    "            return print(response.status_code)\n",
    "   \n",
    "    def get_links(self) -> list:\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class NYTimesSpider(scrapy.Spider) -> None:\n",
    "    name = \"nytimesspider\"\n",
    "    allowed_domains = \"nytimes.com\"\n",
    "    start_urls = [\"https://nytimes.com/media\"]\n",
    "    \n",
    "    def parse(self, reponse):\n",
    "        articles = response.css('li.css-18yolpw')\n",
    "        \n",
    "        for book in books:\n",
    "            yield{\n",
    "                'headline':article.css('a h3::text').get(),\n",
    "                'summary':articles.css('p.css-1pga48a.e15t083i1::text').get(),\n",
    "                'url': f'https://nytimes.com{article.css('a::attr(href)').get()}'\n",
    "            }\n",
    "\n",
    "    \n",
    "    #news in NYT is stored in an ol\n",
    "    #Divs have ids for each page ex. <div id=\"page-1\"></div>\n",
    "    #Inside the divs are li's that contain the headlines\n",
    "    #Urls are stored in the a class of each li class\n",
    "    #Headlines are stored in an h3 class\n",
    "    #Dates are stored in a div with class=\"css-agsgss e15t083i3\"\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def main():\n",
    "    example = Extractor([\"https://www.nytimes.com/media\"])\n",
    "    process = CrawlerProcess(example.create_client.headers)\n",
    "    process.crawl(NYTimesSpider)\n",
    "    process.start()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a06dcaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1665506888.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    scrapy startproject newslinkscraper\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy startproject newslinkscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b71e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
