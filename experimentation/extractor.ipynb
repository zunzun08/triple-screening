{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import asyncio\n",
    "from scrapy.crawler import Crawler\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkManager:\n",
    "    def __init__(self):\n",
    "        self.headers ={\n",
    "        'User-Agent': 'Mozilla/5.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
    "    }\n",
    "\n",
    "        self.proxies = {'http': \"108.161.135.118\"}\n",
    "\n",
    "    def create_session(self) -> requests.Session:\n",
    "        if not self.proxies:\n",
    "            raise ValueError('no proxy available')\n",
    "        if self.headers is None:\n",
    "            raise ValueError('headers are not valid')\n",
    "        \n",
    "        client = requests.Session()\n",
    "        client.proxies.update(self.proxies)\n",
    "        client.headers.update(self.headers)\n",
    "        return client\n",
    "    \n",
    "    def check_status(self, urls: list) -> None:\n",
    "        for url in urls:\n",
    "            response = self.session.get(url)\n",
    "            print(f'{url} | Status code: {response.status_code}')\n",
    "            return response.status_code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NYTimesSpider(scrapy.Spider):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.config = NetworkManager()\n",
    "        self.session: requests.Session = self.config.create_session()\n",
    "        self.pages_to_parse = pages_to_parse\n",
    "\n",
    "\n",
    "\n",
    "    #Scarapy configs\n",
    "    name = \"nytimesspider\"\n",
    "    allowed_domains = [\"nytimes.com\"]\n",
    "    start_urls = [\"https://nytimes.com/section/business/media\"]\n",
    "\n",
    "\n",
    "\n",
    "    def _get_tokens(self):\n",
    "\n",
    "        try:\n",
    "            r = self.session.get(url=\"https://nytimes.com\")\n",
    "            r.raise_for_status()\n",
    "\n",
    "            if not self._is_valid_response(r):\n",
    "                return None\n",
    "            \n",
    "            if not self.content_validation(r):\n",
    "                return None\n",
    "\n",
    "            #Token extraction\n",
    "\n",
    "            #variables needed\n",
    "            vars_dict = {\n",
    "                \"nyt_token\": None, \n",
    "                \"nyt-app-type\": None, \n",
    "                \"nyt-app-version\": None}\n",
    "\n",
    "            for variables in vars_dict:\n",
    "\n",
    "                #Search\n",
    "                filtered_value  = f'[\"\\']{variables}[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']'\n",
    "                val = re.search(fr'{filtered_value}', r.text)\n",
    "\n",
    "                if val:\n",
    "                    token = val.group(1)\n",
    "                    token = token.strip()\n",
    "                else:\n",
    "                    token = None\n",
    "\n",
    "\n",
    "                vars_dict[variables] = token\n",
    "\n",
    "\n",
    "            return vars_dict\n",
    "\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.log(f\"Request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _is_valid_response(self, response):\n",
    "        if response.status_code == 403:\n",
    "            #Can try alternate headers here\n",
    "\n",
    "            alternate_headers = {}\n",
    "\n",
    "            return False\n",
    "        return 200 <= response.status_code < 300\n",
    "    \n",
    "\n",
    "    def _content_validation(self, response):\n",
    "        if not response.text:\n",
    "            return False\n",
    "        if 'text/html' not in response.headers:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "\n",
    "    #Update headers dynamically\n",
    "    def header_update(self):\n",
    "        header_extension = self._get_tokens()\n",
    "        if header_extension:\n",
    "            self.headers = self.headers.update(header_extension)\n",
    "            #Update session headers\n",
    "            self.session.headers.update(self.headers)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Scrapy config\n",
    "    def start_requests(self, cursor=None):\n",
    "        endpoint = self._request_generator()\n",
    "        if self._check_api_connection(endpoint):\n",
    "            yield scrapy.Request(\n",
    "                url=endpoint,\n",
    "                headers=self.session.headers,\n",
    "                callback=self.parse \n",
    "                )\n",
    "        else:\n",
    "            self.logger.log(\"API Connection Error\")\n",
    "\n",
    "\n",
    "    def _request_generator(self, cursor = None, operation_name: str = 'PersonalizedPackagesQuery') -> str:\n",
    "        variables = {\n",
    "        \"id\":\"/section/business/media\",\n",
    "        \"first\":10,\n",
    "        \"exclusionMode\":\"HIGHLIGHTS_AND_EMBEDDED\",\n",
    "        \"isFetchMore\":False,\n",
    "        \"isTranslatable\":False,\n",
    "        \"isEspanol\":False,\n",
    "        \"highlightsListUri\":\"nyt://per/personalized-list/__null__\",\n",
    "        \"highlightsListFirst\":0,\n",
    "        \"hasHighlightsList\":False,\n",
    "        \"cursor\": cursor\n",
    "        }\n",
    "        extension =  {\"persistedQuery\":{\"version\":1,\"sha256Hash\":\"8334262659d77fc2166184bf897e6d139e437af3a9b84d0c020d3dfcb0f177b8\"}}\n",
    "        \n",
    "        \n",
    "        #formatting\n",
    "        var_query = quote(json.dumps(variables))\n",
    "        extension_query = quote(json.dumps(extension))\n",
    "\n",
    "        api_endpoint = f'https://samizdat-graphql.nytimes.com/graphql/v2?operation_name={operation_name}&variables={var_query}&extension={extension_query}'\n",
    "        \n",
    "        return api_endpoint\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        collection = data['data']['legacyCollection']['collectionsPage']\n",
    "        articles = collection['stream']['edges']\n",
    "\n",
    "        for article in articles:\n",
    "            yield {\n",
    "                'headline':article['node']['headline']['default'],  #Need to get text which is in default=\"headline\"\n",
    "                'summary': article['node']['summary'],\n",
    "                'url': article['node']['url'],\n",
    "                'News Source': article['node']['_typename']\n",
    "            }\n",
    "        \n",
    "        \n",
    "        #Now we need to parse through the pages\n",
    "        #The end paramater will be the new start parameter\n",
    "        start_cursor = collection['stream']['pageInfo']['endCursor']\n",
    "        current_page = getattr(response.meta, 'page', 1)\n",
    "        \n",
    "        if start_cursor and current_page < self.pages_to_parse:\n",
    "            next_endpoint = self._request_generator(cursor=start_cursor)\n",
    "            \n",
    "            yield scrapy.Request(\n",
    "                url=next_endpoint,\n",
    "                headers=self.session.headers,\n",
    "                callback=self.parse\n",
    "            )\n",
    "            \n",
    "    def _check_api_connection(self, url):\n",
    "        try:\n",
    "            r = self.session.get(url)\n",
    "            if 200 <= r.status_code < 300:\n",
    "                return True\n",
    "            elif 400 <= r.status_code < 500:\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.log(\"API Conncection Dead!\")\n",
    "            return None\n",
    "\n",
    "\n",
    "class RateLimitManager:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if __name__ == \"__main__\":\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b71e3",
   "metadata": {},
   "source": [
    "https://samizdat-graphql.nytimes.com/graphql/v2?operationName=CollectionsQuery&variables=%7B%22id%22%3A%22%2Fsection%2Fbusiness%2Fmedia%22%2C%22first%22%3A10%2C%22exclusionMode%22%3A%22HIGHLIGHTS_AND_EMBEDDED%22%2C%22isFetchMore%22%3Afalse%2C%22isTranslatable%22%3Afalse%2C%22isEspanol%22%3Afalse%2C%22highlightsListUri%22%3A%22nyt%3A%2F%2Fper%2Fpersonalized-list%2F__null__%22%2C%22highlightsListFirst%22%3A0%2C%22hasHighlightsList%22%3Afalse%2C%22cursor%22%3A%22YXJyYXljb25uZWN0aW9uOjIwOQ%3D%3D%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%228334262659d77fc2166184bf897e6d139e437af3a9b84d0c020d3dfcb0f177b8%22%7D%7D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
