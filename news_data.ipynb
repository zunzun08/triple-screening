{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f448925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from dateutil import parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420065e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#For Desktop \n",
    "#s.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "#For Desktop\n",
    "#login(token=os.getenv(\"HF_MASTER_KEY\"))\n",
    "\n",
    "#For Macbook\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"MACBOOK_HF_KEY\"))\n",
    "\n",
    "# Clear conflicting environment variables\n",
    "#if 'PYTHONPATH' in os.environ:\n",
    "    #del os.environ['PYTHONPATH']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0849b7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/02 15:06:47 WARN Utils: Your hostname, MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 10.42.73.3 instead (on interface en0)\n",
      "25/07/02 15:06:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/02 15:06:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/02 15:06:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Test run with Reuters and Ashraq financial news dataset\n",
    "reuters_spark = SparkSession.builder.appName(\"ReutersNews\").getOrCreate()\n",
    "finnews_spark = SparkSession.builder.appName('FinNews').getOrCreate()\n",
    "\n",
    "# Dictionary mapping spark sessions to dataset names\n",
    "spark_dataset_mapping = {\n",
    "    reuters_spark: \"danidanou/Reuters_Financial_News\",\n",
    "    finnews_spark: \"ashraq/financial-news-articles\"\n",
    "}\n",
    "\n",
    "def ds_to_spark(spark_session, dataset_name, dataset_size=10000):\n",
    "    \"\"\"\n",
    "    Convert a HuggingFace dataset to a Spark DataFrame\n",
    "    \n",
    "    Args:\n",
    "        spark_session: Active Spark session\n",
    "        dataset_name: HuggingFace dataset name/path\n",
    "        dataset_size: Number of rows to load (default: 10000)\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame or None if failed\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "    \n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load HuggingFace dataset with size limit\n",
    "        hf_df = load_dataset(dataset_name, split=f'train[:{dataset_size}]')\n",
    "        \n",
    "        # Convert to pandas\n",
    "        pd_df = hf_df.to_pandas()\n",
    "        \n",
    "        print(f\"DF shape: {pd_df.shape}\")\n",
    "        \n",
    "        # Check and clean columns\n",
    "        for col in pd_df.columns:\n",
    "            if pd_df[col].dtype == 'object':\n",
    "                # Get a sample of non-null values\n",
    "                non_null_values = pd_df[col].dropna()\n",
    "                \n",
    "                if len(non_null_values) > 0:\n",
    "                    # Check the first few values to determine type\n",
    "                    sample_value = non_null_values.iloc[0]\n",
    "                    \n",
    "                    if isinstance(sample_value, (list, dict)):\n",
    "                        # Convert complex types to string\n",
    "                        pd_df[col] = pd_df[col].apply(lambda x: str(x) if x is not None else \"\")\n",
    "                        print(f\"Converted col '{col}' to string\")\n",
    "        \n",
    "        # Fill null values\n",
    "        pd_df = pd_df.fillna(\"\")\n",
    "        \n",
    "        try:\n",
    "            # Convert pandas DataFrame to list of Row objects for compatibility\n",
    "            records = pd_df.to_dict('records')\n",
    "            spark_df = spark_session.createDataFrame(records)\n",
    "            print(f\"Successfully created Spark DataFrame for {dataset_name}\")\n",
    "            print(\"Schema:\")\n",
    "            spark_df.printSchema()\n",
    "            return spark_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame for {dataset_name}: {e}\")\n",
    "            \n",
    "            # Alternative approach: Define explicit schema and use records\n",
    "            print(\"Trying with explicit string schema and records conversion...\")\n",
    "            \n",
    "            try:\n",
    "                # Create schema with all string types\n",
    "                string_schema = StructType([\n",
    "                    StructField(col, StringType(), True) for col in pd_df.columns\n",
    "                ])\n",
    "                \n",
    "                # Convert all columns to string\n",
    "                pd_df_str = pd_df.astype(str)\n",
    "                \n",
    "                # Convert to records format\n",
    "                records = pd_df_str.to_dict('records')\n",
    "                \n",
    "                # Create Spark DataFrame with explicit schema\n",
    "                spark_df = spark_session.createDataFrame(records, schema=string_schema)\n",
    "                print(f\"Successfully created Spark DataFrame with string schema for {dataset_name}\")\n",
    "                print(\"Schema:\")\n",
    "                spark_df.printSchema()\n",
    "                return spark_df\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"Failed even with string schema and records: {e2}\")\n",
    "                \n",
    "                # Last resort: Manual row creation\n",
    "                print(\"Trying manual row creation...\")\n",
    "                try:\n",
    "                    from pyspark.sql import Row\n",
    "                    \n",
    "                    # Create Row objects manually\n",
    "                    Row = Row(*pd_df.columns)\n",
    "                    rows = [Row(*[str(val) for val in row]) for row in pd_df.values]\n",
    "                    \n",
    "                    spark_df = spark_session.createDataFrame(rows)\n",
    "                    print(f\"Successfully created Spark DataFrame with manual row creation for {dataset_name}\")\n",
    "                    print(\"Schema:\")\n",
    "                    spark_df.printSchema()\n",
    "                    return spark_df\n",
    "                    \n",
    "                except Exception as e3:\n",
    "                    print(f\"All methods failed for {dataset_name}: {e3}\")\n",
    "                    return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694ee32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 411547 entries, 0 to 411600\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   title   411547 non-null  object\n",
      " 1   Date    105359 non-null  object\n",
      " 2   url     411547 non-null  object\n",
      " 3   text    411547 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "reuters_df = load_dataset(\"danidanou/Reuters_Financial_News\")['train'].to_pandas()\n",
    "finnews_df = load_dataset(\"ashraq/financial-news-articles\")['train'].to_pandas()\n",
    "\n",
    "reuters_df = reuters_df.rename(columns={\n",
    "    'Article': 'text',\n",
    "    'Link': 'url',\n",
    "    'Headline': 'title'\n",
    "    })\n",
    "\n",
    "reuters_df = reuters_df.drop(columns=['Journalists', '__index_level_0__', 'Summary'])\n",
    "\n",
    "#Merging the df\n",
    "merged_df = pd.concat([reuters_df, finnews_df])\n",
    "\n",
    "#Shuffling the rows\n",
    "merged_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Removing duplicates based on url\n",
    "merged_df = merged_df.drop_duplicates(subset=['url'], keep='first')\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98a03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristianzuniga/Library/Python/3.10/lib/python/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/cristianzuniga/Library/Python/3.10/lib/python/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>news_medium</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relief, anger, fear greet autos bailout in Det...</td>\n",
       "      <td>2008-12-19</td>\n",
       "      <td>http://www.reuters.com/article/2008/12/20/us-a...</td>\n",
       "      <td>But mixed with relief was fear about what fre...</td>\n",
       "      <td>3935</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIMCO's Gross sees fed funds at 3 percent by m...</td>\n",
       "      <td>2008-01-08</td>\n",
       "      <td>http://www.reuters.com/article/2008/01/08/us-u...</td>\n",
       "      <td>The federal funds rate, the Fed's key policy ...</td>\n",
       "      <td>1324</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRIEF-Indra Buys Paradigma, Consulting Firm Of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reuters.com/article/brief-indra-bu...</td>\n",
       "      <td>Jan 17 (Reuters) - INDRA:\\n* BUYS PARADIGMA, C...</td>\n",
       "      <td>131</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joost names former Cisco executive new CEO</td>\n",
       "      <td>2007-06-05</td>\n",
       "      <td>http://www.reuters.com/article/2007/06/05/us-j...</td>\n",
       "      <td>Joost, launched late last year by the founder...</td>\n",
       "      <td>1351</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wall Street Week Ahead: It's earnings versus E...</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>http://www.reuters.com/article/2012/01/13/us-u...</td>\n",
       "      <td>Bank stocks will probably once again be a pri...</td>\n",
       "      <td>3359</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411596</th>\n",
       "      <td>BRIEF-Medicalsystem Biotechnology announces ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reuters.com/article/brief-medicals...</td>\n",
       "      <td>April 20(Reuters) - Medicalsystem Biotechnolog...</td>\n",
       "      <td>235</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411597</th>\n",
       "      <td>Short sellers target Apple supplier IQE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reuters.com/article/iqe-shortselli...</td>\n",
       "      <td>January 30, 2018 / 3:09 PM / Updated an hour a...</td>\n",
       "      <td>3601</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411598</th>\n",
       "      <td>BRIEF-Taiwan Leader Biotech sets subscription ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reuters.com/article/brief-taiwan-l...</td>\n",
       "      <td>March 7 (Reuters) - Taiwan Leader Biotech Corp...</td>\n",
       "      <td>422</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411599</th>\n",
       "      <td>UK's Cameron calls for new press regulation sy...</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>http://www.reuters.com/article/2011/07/08/us-n...</td>\n",
       "      <td>\"I believe we need a new system entirely, it ...</td>\n",
       "      <td>831</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411600</th>\n",
       "      <td>Billionaire Hedge-Fund Trader Alan Howard Prof...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.wsj.com/articles/billionaire-hedge...</td>\n",
       "      <td>A secretive hedge fund personally managed by b...</td>\n",
       "      <td>531</td>\n",
       "      <td>wsj</td>\n",
       "      <td>articles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411547 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title        Date  \\\n",
       "0       Relief, anger, fear greet autos bailout in Det...  2008-12-19   \n",
       "1       PIMCO's Gross sees fed funds at 3 percent by m...  2008-01-08   \n",
       "2       BRIEF-Indra Buys Paradigma, Consulting Firm Of...         NaN   \n",
       "3              Joost names former Cisco executive new CEO  2007-06-05   \n",
       "4       Wall Street Week Ahead: It's earnings versus E...  2012-01-13   \n",
       "...                                                   ...         ...   \n",
       "411596  BRIEF-Medicalsystem Biotechnology announces ch...         NaN   \n",
       "411597            Short sellers target Apple supplier IQE         NaN   \n",
       "411598  BRIEF-Taiwan Leader Biotech sets subscription ...         NaN   \n",
       "411599  UK's Cameron calls for new press regulation sy...  2011-07-08   \n",
       "411600  Billionaire Hedge-Fund Trader Alan Howard Prof...         NaN   \n",
       "\n",
       "                                                      url  \\\n",
       "0       http://www.reuters.com/article/2008/12/20/us-a...   \n",
       "1       http://www.reuters.com/article/2008/01/08/us-u...   \n",
       "2       https://www.reuters.com/article/brief-indra-bu...   \n",
       "3       http://www.reuters.com/article/2007/06/05/us-j...   \n",
       "4       http://www.reuters.com/article/2012/01/13/us-u...   \n",
       "...                                                   ...   \n",
       "411596  https://www.reuters.com/article/brief-medicals...   \n",
       "411597  https://www.reuters.com/article/iqe-shortselli...   \n",
       "411598  https://www.reuters.com/article/brief-taiwan-l...   \n",
       "411599  http://www.reuters.com/article/2011/07/08/us-n...   \n",
       "411600  https://www.wsj.com/articles/billionaire-hedge...   \n",
       "\n",
       "                                                     text  text_length  \\\n",
       "0        But mixed with relief was fear about what fre...         3935   \n",
       "1        The federal funds rate, the Fed's key policy ...         1324   \n",
       "2       Jan 17 (Reuters) - INDRA:\\n* BUYS PARADIGMA, C...          131   \n",
       "3        Joost, launched late last year by the founder...         1351   \n",
       "4        Bank stocks will probably once again be a pri...         3359   \n",
       "...                                                   ...          ...   \n",
       "411596  April 20(Reuters) - Medicalsystem Biotechnolog...          235   \n",
       "411597  January 30, 2018 / 3:09 PM / Updated an hour a...         3601   \n",
       "411598  March 7 (Reuters) - Taiwan Leader Biotech Corp...          422   \n",
       "411599   \"I believe we need a new system entirely, it ...          831   \n",
       "411600  A secretive hedge fund personally managed by b...          531   \n",
       "\n",
       "       news_medium news_type  \n",
       "0          reuters   article  \n",
       "1          reuters   article  \n",
       "2          reuters   article  \n",
       "3          reuters   article  \n",
       "4          reuters   article  \n",
       "...            ...       ...  \n",
       "411596     reuters   article  \n",
       "411597     reuters   article  \n",
       "411598     reuters   article  \n",
       "411599     reuters   article  \n",
       "411600         wsj  articles  \n",
       "\n",
       "[411547 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA CLEANING \n",
    "\n",
    "\n",
    "merged_df['text_length'] = merged_df['text'].apply(lambda text: len(text))\n",
    "\n",
    "merged_df['news_medium'] = merged_df['url'].str.extract(r'https?://([^/]+)')[0].str.replace('www.', '', regex=False).str[:-4]\n",
    "\n",
    "#Standardizing dates and getting more date information from url\n",
    "\n",
    "#Turning string dates to YYYY-MM-DD HH:MM:SS Format\n",
    "merged_df['Date'] = merged_df['Date'].apply(lambda date: parser.parse(date) if pd.notna(date) else np.NaN)\n",
    "\n",
    "\n",
    "#Getting dates from URL\n",
    "\n",
    "\n",
    "url_dates = merged_df['url'].str.extract(r'/(\\d{4})[/\\-](\\d{2})[/\\-](\\d{2})/').dropna().T.apply('-'.join)\n",
    "\n",
    "merged_df['Date'] = merged_df['Date'].fillna(url_dates)\n",
    "\n",
    "\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], errors='coerce')\n",
    "\n",
    "merged_df['Date'] = merged_df['Date'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "#News article types\n",
    "merged_df['news_type'] = merged_df['url'].str.extract(r'/([a-zA-Z]+)/')[0]\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34107ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reuters_df = ds_to_spark(reuters_spark, \"danidanou/Reuters_Financial_News\", 30000)\n",
    "#finnews_df = ds_to_spark(finnews_spark, \"ashraq/financial-news-articles\", 30000)\n",
    "\n",
    "# Check if successful\n",
    "#if reuters_df:\n",
    "    #print(\"Reuters dataset loaded successfully\")\n",
    "    #reuters_df.show(5)\n",
    "\n",
    "#if finnews_df:\n",
    "    #print(\"Financial News dataset loaded successfully\") \n",
    "    #finnews_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e4a8b",
   "metadata": {},
   "source": [
    "Below are methods to push the data to Supabase DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364a0ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not resolve hostname: db.cxxzifmsmlxqllnfcurt.supabase.co\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "hostname = \"db.cxxzifmsmlxqllnfcurt.supabase.co\"\n",
    "try:\n",
    "    socket.gethostbyname(hostname)\n",
    "    print(f\"Hostname {hostname} resolved successfully\")\n",
    "except socket.gaierror:\n",
    "    print(f\"Could not resolve hostname: {hostname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93f440a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mediciroasting19'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"SB_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5230c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/02 17:34:41 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 666992 ms exceeds timeout 120000 ms\n",
      "25/07/02 17:34:41 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "\n",
    "db_pass = os.getenv(\"SB_PASS\")\n",
    "connection = f\"postgresql://postgres.cxxzifmsmlxqllnfcurt:{db_pass}@aws-0-us-east-2.pooler.supabase.com:5432/postgres\"\n",
    "\n",
    "conn = create_engine(connection)\n",
    "\n",
    "merged_df.drop(columns=['text']).to_sql('pretrain_data', con=conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf0ac0",
   "metadata": {},
   "source": [
    "Below are methods to tokenize the merged dataset and push the data to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5efdb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=['text']).to_csv('pretrain_data.csv', sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f27ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 411547 entries, 0 to 411600\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   title        411547 non-null  object\n",
      " 1   Date         212531 non-null  object\n",
      " 2   url          411547 non-null  object\n",
      " 3   text         411547 non-null  object\n",
      " 4   text_length  411547 non-null  int64 \n",
      " 5   news_medium  411547 non-null  object\n",
      " 6   news_type    335698 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 25.1+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28ec3b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>news_medium</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relief, anger, fear greet autos bailout in Det...</td>\n",
       "      <td>2008-12-19</td>\n",
       "      <td>http://www.reuters.com/article/2008/12/20/us-a...</td>\n",
       "      <td>But mixed with relief was fear about what fre...</td>\n",
       "      <td>3935</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIMCO's Gross sees fed funds at 3 percent by m...</td>\n",
       "      <td>2008-01-08</td>\n",
       "      <td>http://www.reuters.com/article/2008/01/08/us-u...</td>\n",
       "      <td>The federal funds rate, the Fed's key policy ...</td>\n",
       "      <td>1324</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRIEF-Indra Buys Paradigma, Consulting Firm Of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reuters.com/article/brief-indra-bu...</td>\n",
       "      <td>Jan 17 (Reuters) - INDRA:\\n* BUYS PARADIGMA, C...</td>\n",
       "      <td>131</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joost names former Cisco executive new CEO</td>\n",
       "      <td>2007-06-05</td>\n",
       "      <td>http://www.reuters.com/article/2007/06/05/us-j...</td>\n",
       "      <td>Joost, launched late last year by the founder...</td>\n",
       "      <td>1351</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wall Street Week Ahead: It's earnings versus E...</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>http://www.reuters.com/article/2012/01/13/us-u...</td>\n",
       "      <td>Bank stocks will probably once again be a pri...</td>\n",
       "      <td>3359</td>\n",
       "      <td>reuters</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        Date  \\\n",
       "0  Relief, anger, fear greet autos bailout in Det...  2008-12-19   \n",
       "1  PIMCO's Gross sees fed funds at 3 percent by m...  2008-01-08   \n",
       "2  BRIEF-Indra Buys Paradigma, Consulting Firm Of...         NaN   \n",
       "3         Joost names former Cisco executive new CEO  2007-06-05   \n",
       "4  Wall Street Week Ahead: It's earnings versus E...  2012-01-13   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.reuters.com/article/2008/12/20/us-a...   \n",
       "1  http://www.reuters.com/article/2008/01/08/us-u...   \n",
       "2  https://www.reuters.com/article/brief-indra-bu...   \n",
       "3  http://www.reuters.com/article/2007/06/05/us-j...   \n",
       "4  http://www.reuters.com/article/2012/01/13/us-u...   \n",
       "\n",
       "                                                text  text_length news_medium  \\\n",
       "0   But mixed with relief was fear about what fre...         3935     reuters   \n",
       "1   The federal funds rate, the Fed's key policy ...         1324     reuters   \n",
       "2  Jan 17 (Reuters) - INDRA:\\n* BUYS PARADIGMA, C...          131     reuters   \n",
       "3   Joost, launched late last year by the founder...         1351     reuters   \n",
       "4   Bank stocks will probably once again be a pri...         3359     reuters   \n",
       "\n",
       "  news_type  \n",
       "0   article  \n",
       "1   article  \n",
       "2   article  \n",
       "3   article  \n",
       "4   article  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f49b1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuters\n",
      "cnbc\n",
      "wsj\n",
      "in.reuters\n",
      "uk.reuters\n",
      "fortune\n",
      "jp.wsj\n",
      "live.wsj\n",
      "blogs.wsj\n",
      "it.reuters\n",
      "graphics.wsj\n",
      "cn.reuters\n",
      "quotes.wsj\n",
      "chinese.wsj\n",
      "\n",
      "article\n",
      "video\n",
      "nan\n",
      "articles\n",
      "moneybeat\n",
      "puzzle\n",
      "cio\n",
      "news\n",
      "riskandcompliance\n",
      "economics\n",
      "podcasts\n",
      "washwire\n",
      "cfo\n",
      "longform\n",
      "glider\n",
      "briefly\n",
      "id\n",
      "livecoverage\n",
      "graphics\n",
      "experts\n",
      "frontiers\n",
      "advertorial\n",
      "investigates\n",
      "amp\n",
      "dailyshot\n",
      "index\n",
      "ALLY\n",
      "KSS\n"
     ]
    }
   ],
   "source": [
    "#Investigating some possible merging\n",
    "for entry in merged_df.news_medium.unique():\n",
    "    print(entry)\n",
    "print()\n",
    "for entry in merged_df.news_type.unique():\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54c7614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_subsection(entry):\n",
    "    entry_list = entry.split('.')\n",
    "    if len(entry_list) > 1:\n",
    "        return entry_list[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "merged_df['news_source'] = merged_df['news_medium'].str.split('.').str[-1]\n",
    "merged_df['news_subsection'] = merged_df['news_medium'].apply(find_subsection)\n",
    "merged_df['news_type'] = merged_df['news_type'].replace({'articles': 'article'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7e65256",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=['text']).to_csv('pretrain_data.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd1c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll do 3 methods the first will be to take the first 512 words of an article (if it has it), combine into a column for a dataframe\n",
    "#The second method will be to take the middle 512 words (if available) of an article and combine into a column\n",
    "#The third will be to take the first 2000 words of an article, combine into a column \n",
    "\n",
    "from pyspark.sql.functions import substring, col, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "\n",
    "#Method one\n",
    "first_512_df = merged_df[merged_df['text_length'] >= 512]\n",
    "\n",
    "#Method two\n",
    "\n",
    "#Making a function to define middle of column\n",
    "middle_512_df = merged_df\n",
    "middle_512_df['text'] = merged_df['text'].apply(\n",
    "    lambda text: text[int((len(text) - 512) / 2):int(len(text) - (len(text) - 512) / 2)]\n",
    ")\n",
    "\n",
    "#Third_method\n",
    "first_4096_df = merged_df[merged_df[\"text_length\"] >= 4096]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "948ddca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf3afe7a1f4f24933fcfd9e5f002ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/314651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f19f06f2724947975eca4ab2ed4372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/411601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee47a987c1e544db93602028a2563200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The third method will feed into a longform tranfsformer for pretraining\n",
    "#The second method and first method will feed into a DistilBERT for pretraining\n",
    "#We'll use a one lstm that will train on the first 512 words (benchmark, to be added later)\n",
    "\n",
    "\n",
    "#Bert tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "lf_tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "datasets_pd = {\n",
    "    'first_512': first_512_df,\n",
    "    'middle_512': middle_512_df, \n",
    "    'first_4096': first_4096_df\n",
    "}\n",
    "\n",
    "    \n",
    "# Tokenize\n",
    "datasets_hf = {\n",
    "    name: Dataset.from_pandas(df) \n",
    "    for name, df in datasets_pd.items()\n",
    "}\n",
    "\n",
    "\n",
    "configs = [\n",
    "    {'name': 'first_512', 'tokenizer': bert_tokenizer, 'max_length': 512},\n",
    "    {'name': 'middle_512', 'tokenizer': bert_tokenizer, 'max_length': 512},\n",
    "    {'name': 'first_4096', 'tokenizer': lf_tokenizer, 'max_length': 4096}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_datasets = {}\n",
    "\n",
    "\n",
    "for config in configs:\n",
    "    tokenized_datasets[config['name']] = datasets_hf[config['name']].map(\n",
    "        lambda examples: config['tokenizer'](\n",
    "            examples['text'],  # adjust column name as needed\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=config['max_length'],\n",
    "            return_tensors='pt'\n",
    "        ),\n",
    "        batched=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b454c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71fb78ff9eb40969458febad2aeca16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c9cb51f8a84ffaa0d42d9e9a3ad24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ab00b973004633ad9996c42d1bb58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9b7f4ec0f041e3ba798e02b54a0e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af724a0041e4454bbdffda81ea5310ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c1ecd1cb6647c981000c72d91e9587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/369 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512fbd6610fd4e5c935c7ae5bdb93cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198fd9bb92f5451b931c74c734805a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/138 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5fd16d20c5480591f6c12eb13fb389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/138 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7238fbddf4024e10b5d575deaa684916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/138 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b607a353024254babc08136026cda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2840518fba47489f913f3a2f84a7b98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783b28393ba24f978c0b0e36a430e818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4364a1cdf242f2a6f6e262a958bb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e587673c94481990813df149246cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af8ea9dd5f6485e8acacb1f21d3dedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/369 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pushing tokenized datasets to huggingface\n",
    "\n",
    "for dataset in tokenized_datasets.keys():\n",
    "    tokenized_datasets[dataset].push_to_hub(f\"Czunzun/Financial_news_{dataset}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
